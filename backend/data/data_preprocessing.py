import pandas as pd
import os
from tqdm import tqdm
from validation_checks import validate_schema, validate_foreign_keys
from schemas import MOVIES_SCHEMA, RATINGS_SCHEMA
from fetch_tmdb_metadata import fetch_and_cache_tmdb_metadata

# âœ… Define Paths
PROCESSED_DATA_PATH = "backend/data/processed/"
RAW_DATA_PATH = "backend/data/raw/"
VALID_LINKS_FILE = os.path.join(PROCESSED_DATA_PATH, "valid_links.csv")
MOVIES_FILE = os.path.join(RAW_DATA_PATH, "movies.csv")
RATINGS_FILE = os.path.join(RAW_DATA_PATH, "ratings.csv")
LINKS_FILE = os.path.join(RAW_DATA_PATH, "links.csv")
TMDB_METADATA_CACHE_FILE = os.path.join(PROCESSED_DATA_PATH, "tmdb_metadata_cache.csv")

# âœ… Ensure processed folder exists
os.makedirs(PROCESSED_DATA_PATH, exist_ok=True)

# âœ… Load Valid TMDb IDs (WITH FALLBACK)
print("ğŸ” Loading valid TMDb IDs...")
try:
    valid_links = pd.read_csv(VALID_LINKS_FILE)
    print("âœ… Using existing valid_links.csv")
except FileNotFoundError:
    print("âš ï¸ valid_links.csv not found. Using all links from links.csv")
    valid_links = pd.read_csv(LINKS_FILE)
    valid_links = valid_links[["movieId", "tmdbId"]]
    valid_links.to_csv(VALID_LINKS_FILE, index=False)
    print(f"ğŸ“„ Created placeholder {VALID_LINKS_FILE}")

# âœ… Load raw datasets
print("ğŸ” Loading raw movies and ratings...")
movies = pd.read_csv(MOVIES_FILE)
ratings = pd.read_csv(RATINGS_FILE)
print(f"ğŸ“Š Total movies loaded: {len(movies)}")

# âœ… Merge Valid TMDb IDs with Movies Dataset
print("ğŸ”„ Merging valid TMDb IDs with movies...")
movies = movies.merge(valid_links, on="movieId", how="left")

# âœ… Extract year and tmdbId as floats (nullable)
movies["year"] = pd.to_numeric(movies["title"].str.extract(r"\((\d{4})\)")[0], errors="coerce")
movies["tmdbId"] = pd.to_numeric(movies["tmdbId"], errors="coerce")

# âœ… Filter incomplete movies
before_filtering = len(movies)
movies = movies[movies["tmdbId"].notna() & movies["year"].notna()]
after_filtering = len(movies)
print(f"ğŸ“‰ Movies after filtering for valid TMDb ID and year: {after_filtering} (removed {before_filtering - after_filtering})")

# âœ… Convert to non-nullable integers after filtering
movies = movies[movies["tmdbId"].notna()]
movies["tmdbId"] = movies["tmdbId"].astype("int64")
movies["valid_tmdb"] = True
movies["year"] = movies["year"].astype("int64")

# âœ… Fetch metadata from TMDb and merge
print("ğŸŒ Fetching metadata from TMDb for valid movies...")
metadata = fetch_and_cache_tmdb_metadata(movies)
movies = pd.merge(movies, metadata, on="tmdbId", how="left", suffixes=('', '_meta'))

# âœ… Schema Validation (AFTER metadata is included)
schema_errors = validate_schema(movies, MOVIES_SCHEMA)
if schema_errors:
    print("ğŸš¨ Schema errors in movies data:")
    for error in schema_errors:
        print(f" - {error}")
    exit()

# âœ… Drop rows missing any required metadata
required_columns = ["title", "year", "genres", "tmdbId", "description", "poster_url", "age_restriction"]
movies = movies.dropna(subset=required_columns)

# âœ… Filter ratings for valid movies only
print("ğŸ”„ Filtering ratings for valid movies...")
ratings = ratings[ratings["movieId"].isin(movies["movieId"])]

# âœ… Foreign Key Validation (optional)
rating_errors = validate_foreign_keys(ratings, movies)
if rating_errors:
    print("ğŸš¨ Foreign key errors:")
    for error in rating_errors:
        print(f" - {error}")

# âœ… Save Cleaned Data
CLEAN_MOVIES_FILE = os.path.join(PROCESSED_DATA_PATH, "clean_movies.csv")
CLEAN_RATINGS_FILE = os.path.join(PROCESSED_DATA_PATH, "clean_ratings.csv")

movies.to_csv(CLEAN_MOVIES_FILE, index=False)
ratings.to_csv(CLEAN_RATINGS_FILE, index=False)

# âœ… Summary
print(f"\nâœ… Processed Movies: {len(movies)}")
print(f"âœ… Processed Ratings: {len(ratings)}")
print(f"ğŸ“‚ Cleaned movies saved to: {CLEAN_MOVIES_FILE}")
print(f"ğŸ“‚ Cleaned ratings saved to: {CLEAN_RATINGS_FILE}")
